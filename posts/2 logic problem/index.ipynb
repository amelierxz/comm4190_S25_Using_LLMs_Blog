{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9d3955-4bcd-42f0-ac51-4968d17ee26b",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Creating an interesting word logic problem: Can AI solve it properly?\"\n",
    "description: \"Can AI help me create a brain teaser of sorts focused on word logic, and then solve it like a human would? (Trying to figure out how to use this blog)\"\n",
    "author: \"Amelie Zhang\"\n",
    "date: \"2/2/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - word problem\n",
    "  - logic\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da45d34-fc8a-4536-b91b-d476e1e009c6",
   "metadata": {},
   "source": [
    "When we first started playing around with LLMs and seeing if they could properly communicate with humans one of my initial instincts was to see if they could think like humans. Ever since I was a child, something I really enjoyed doing was solving logic puzzles - like those you always found on Youtube made by TED or some other channel. So, I wanted to take this topic one step further and see if AI would be able to create a problem itself and then solve it.\n",
    "\n",
    "Thus, what I did was ask GPT to create a logic problem for me, inspired by problems where you are given information that can be both true or false, and have to deduce the final answer. After creating the problem, I then fed it into a new window in GPT and tried to see if it could be solved. \n",
    "\n",
    "Obviously, I also enjoyed myself and had fun solving it as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58618eb-dd92-483b-a802-cf2083e61d6d",
   "metadata": {},
   "source": [
    "# A logic problem for LLMs \n",
    "\n",
    "* Read the following and see if you can answer the question:\n",
    "\n",
    ">### The Stolen Key (created by GPT 4o)\n",
    "\n",
    "Four friends—Alex, Ben, Cara, and Dana—were the only people in a locked house when the front door key went missing. Each of them made a statement:  \n",
    "\n",
    "- **Alex**: \"I didn’t take the key.\"  \n",
    "- **Ben**: \"Cara took the key.\"  \n",
    "- **Cara**: \"Dana is lying.\"  \n",
    "- **Dana**: \"Ben is lying.\"  \n",
    "\n",
    "Only one of them is telling the truth. Who stole the key?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841dc06f-cdf2-4a96-a11a-51fc8e856c69",
   "metadata": {},
   "source": [
    "<img src=\"a.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4f8ce-0eb8-47b7-ab32-026a9efd5f34",
   "metadata": {},
   "source": [
    "# How do LLMs response to the prompt?\n",
    "\n",
    "### GPT4o\n",
    "\n",
    "Let's analyze the statements logically:\n",
    "\n",
    "- Only **one** person is telling the truth.\n",
    "- If **Ben** were telling the truth (\"Cara took the key\"), then all other statements must be false.\n",
    "  - Alex saying \"I didn’t take the key\" would be false → meaning Alex **did** take the key. This contradicts Ben's claim.\n",
    "  - Therefore, Ben is **lying**.\n",
    "\n",
    "Since Ben is lying, Cara **did not** take the key. That means we must find another truth-teller.\n",
    "\n",
    "- If **Cara** were telling the truth (\"Dana is lying\"), then:\n",
    "  - Dana saying \"Ben is lying\" would be false, meaning Ben is **telling the truth**.\n",
    "  - But we already established that Ben was lying, so this contradicts our assumption.\n",
    "\n",
    "Since Cara cannot be the truth-teller either, let’s check Dana.\n",
    "\n",
    "- If **Dana** were telling the truth (\"Ben is lying\"), then:\n",
    "  - Ben’s statement (\"Cara took the key\") is false → meaning **Cara did NOT take the key**.\n",
    "  - That makes Alex’s statement (\"I didn’t take the key\") false → meaning **Alex DID take the key**.\n",
    "\n",
    "Since this setup is consistent and only one truth-teller exists, the **key was stolen by Alex**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e955c8f-4dff-4f3e-a80d-a394465a6b69",
   "metadata": {},
   "source": [
    "### GPT4o1\n",
    "\n",
    "**Answer: Alex took the key.**\n",
    "\n",
    "Here’s a quick way to see why:\n",
    "\n",
    "1. **Assume Dana is telling the truth** (“Ben is lying”).\n",
    "2. If Ben is lying, then “Cara took the key” is false, so Cara did **not** take the key.\n",
    "3. Because only one statement can be true, Alex’s and Cara’s statements must be false:\n",
    "   - Alex’s statement “I didn’t take the key” is false, so Alex **did** take the key.\n",
    "   - Cara’s statement “Dana is lying” is false, meaning Dana is indeed telling the truth.\n",
    "\n",
    "All conditions are satisfied with Dana telling the truth and Alex as the thief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba24407-95b7-414f-bb2b-21fd084fa258",
   "metadata": {},
   "source": [
    "### Analysis of LLM Reasoning\n",
    "This problem is an example of how LLMs process logical constraints. Given the one-truth constraint, LLMs systematically eliminate contradictions to arrive at a valid solution.\n",
    "\n",
    "Notably, both GPT-4o and GPT-4o1 reached the same answer through slightly different reasoning:\n",
    "\n",
    "GPT-4o used a step-by-step contradiction approach, testing each individual’s truthfulness and discarding inconsistencies.\n",
    "GPT-4o1 opted for a more streamlined deduction, assuming Dana’s truthfulness and resolving the logical consequences.\n",
    "This exercise highlights LLMs’ ability to process deductive logic, truth-value constraints, and contradictions, making them effective at solving structured reasoning puzzles.\n",
    "\n",
    "Would another LLM produce the same answer? Would performance change if the statements were reworded? Testing across different models could reveal nuances in AI logical reasoning and bias towards specific deduction styles.\n",
    "\n",
    "What do you think—should we test other AI models on this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd4985b-3337-4a17-abee-b6d561c0a810",
   "metadata": {},
   "source": [
    "## My thoughts:\n",
    "\n",
    "I found this super interesting after trying to make this logic problem out of boredom - and attempting to capture how AI can think through a reasoning problem that humans typically attack in a non-linear fashion. While both different GPT models ended up getting the right result, I was fasicinated by the different methods that the two models used, that were logical but did not at all follow the same framework.\n",
    "\n",
    "In the future, I really want to experiment more with how to use AI to attack these sorts of reasoning problems that may not have a clear direct answer, and think about the actual thinking process of the AI LLM behind it.\n",
    "\n",
    "If this problem were to get more complicated, I'd be curious about how the AI could form strings of thought that would mimic the sequential way humans tend to think about problems."
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
